{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the following packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from hyperopt import STATUS_OK, Trials, fmin, hp, tpe\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import mlflow\n",
    "from mlflow.models import infer_signature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set the MLFLOW_TRACKING_URI environment variable to the URI of our MLflow tracking server:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export MLFLOW_TRACKING_URI=\"http://localhost:5000\"\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:8080\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load the dataset and split it into training, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "data = pd.read_csv(\n",
    "    \"https://raw.githubusercontent.com/mlflow/mlflow/master/tests/datasets/winequality-white.csv\",\n",
    "    sep=\";\")\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "train, test = train_test_split(data, test_size=0.25, random_state=42)\n",
    "train_x = train.drop([\"quality\"], axis=1).values\n",
    "train_y = train[[\"quality\"]].values.ravel()\n",
    "test_x = test.drop([\"quality\"], axis=1).values\n",
    "test_y = test[[\"quality\"]].values.ravel()\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(\n",
    "    train_x, train_y, test_size=0.2, random_state=42)\n",
    "signature = infer_signature(train_x, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"train_model\" function uses MLflow to track the parameters, results, and model itself of each trial as a child run. \\n\n",
    "\"objective\" function takes in the hyperparameters and returns the results of the train_model function for that set of hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coefficient of determination (R^2) for regression\n",
    "def r_square(y_true, y_pred):\n",
    "    SS_res =  keras.backend.sum(keras.backend.square(y_true - y_pred)) \n",
    "    SS_tot = keras.backend.sum(keras.backend.square(y_true - keras.backend.mean(y_true))) \n",
    "    return (1 - SS_res/(SS_tot + keras.backend.epsilon()))\n",
    "\n",
    "def train_model(params, data):\n",
    "    # Specify model\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.Input([data['train_x'].shape[1]]))\n",
    "    model.add(keras.layers.Normalization(mean=np.mean(data['train_x']), variance=np.var(data['train_x'])))\n",
    "\n",
    "    # hidden layers\n",
    "    for i in range(params['num_layers']):\n",
    "        model.add(keras.layers.Dense(units=params['hidden_size'], activation='relu'))\n",
    "        \n",
    "    # Output layer\n",
    "    model.add(keras.layers.Dense(1, activation='linear'))\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.SGD(learning_rate=params[\"lr\"], momentum=params[\"momentum\"]),\n",
    "        loss=\"mean_squared_error\", metrics=[keras.metrics.RootMeanSquaredError(), r_square])\n",
    "\n",
    "    # Train model with MLflow tracking\n",
    "    \n",
    "    model.fit(\n",
    "            train_x, train_y, validation_data=(data['valid_x'], data['valid_y']),\n",
    "            epochs=params['epochs'], batch_size=params['batch_size'])\n",
    "    # Evaluate the model\n",
    "    eval_result = model.evaluate(data['valid_x'], data['valid_y'], batch_size=params['batch_size'])\n",
    "    \n",
    "    eval_rmse = eval_result[1]\n",
    "    eval_r2 = eval_result[2]\n",
    "    # Log parameters and results\n",
    "    return {\"loss\": eval_rmse, \"R2\": eval_r2, \"status\": STATUS_OK, \"model\": model}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# do hyperparameter search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "why this? MLflow has an internal capability to log every reading and thus it can record the hyperparameters for each run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lr': 0.034, 'momentum': 0.1, 'epochs': 100, 'num_layers': 4, 'batch_size': 64, 'hidden_size': 3}\n",
      "Epoch 1/100\n",
      "46/46 [==============================] - 1s 4ms/step - loss: 7.0135 - root_mean_squared_error: 2.6483 - r_square: -8.5727 - val_loss: 0.8295 - val_root_mean_squared_error: 0.9107 - val_r_square: -0.0865\n",
      "Epoch 2/100\n",
      "46/46 [==============================] - 0s 1ms/step - loss: 0.7939 - root_mean_squared_error: 0.8910 - r_square: -0.0214 - val_loss: 0.7831 - val_root_mean_squared_error: 0.8849 - val_r_square: -0.0183\n",
      "Epoch 3/100\n",
      "46/46 [==============================] - 0s 999us/step - loss: 0.7881 - root_mean_squared_error: 0.8878 - r_square: -0.0158 - val_loss: 0.7826 - val_root_mean_squared_error: 0.8846 - val_r_square: -0.0174\n",
      "Epoch 4/100\n",
      "46/46 [==============================] - 0s 994us/step - loss: 0.7884 - root_mean_squared_error: 0.8879 - r_square: -0.0186 - val_loss: 0.7831 - val_root_mean_squared_error: 0.8850 - val_r_square: -0.0185\n",
      "Epoch 5/100\n",
      "46/46 [==============================] - 0s 1ms/step - loss: 0.7879 - root_mean_squared_error: 0.8876 - r_square: -0.0137 - val_loss: 0.7820 - val_root_mean_squared_error: 0.8843 - val_r_square: -0.0155\n",
      "Epoch 6/100\n",
      "46/46 [==============================] - 0s 1ms/step - loss: 0.7885 - root_mean_squared_error: 0.8880 - r_square: -0.0179 - val_loss: 0.7826 - val_root_mean_squared_error: 0.8847 - val_r_square: -0.0175\n",
      "Epoch 7/100\n",
      "46/46 [==============================] - 0s 1ms/step - loss: 0.7882 - root_mean_squared_error: 0.8878 - r_square: -0.0157 - val_loss: 0.7820 - val_root_mean_squared_error: 0.8843 - val_r_square: -0.0162\n",
      "Epoch 8/100\n",
      "46/46 [==============================] - 0s 1ms/step - loss: 0.7883 - root_mean_squared_error: 0.8879 - r_square: -0.0174 - val_loss: 0.7824 - val_root_mean_squared_error: 0.8845 - val_r_square: -0.0170\n",
      "Epoch 9/100\n",
      "46/46 [==============================] - 0s 1ms/step - loss: 0.7885 - root_mean_squared_error: 0.8880 - r_square: -0.0198 - val_loss: 0.7831 - val_root_mean_squared_error: 0.8849 - val_r_square: -0.0184\n",
      "Epoch 10/100\n",
      "46/46 [==============================] - 0s 955us/step - loss: 0.7884 - root_mean_squared_error: 0.8879 - r_square: -0.0190 - val_loss: 0.7819 - val_root_mean_squared_error: 0.8843 - val_r_square: -0.0157\n",
      "Epoch 11/100\n",
      "46/46 [==============================] - 0s 920us/step - loss: 0.7882 - root_mean_squared_error: 0.8878 - r_square: -0.0130 - val_loss: 0.7822 - val_root_mean_squared_error: 0.8844 - val_r_square: -0.0166\n",
      "Epoch 12/100\n",
      "46/46 [==============================] - 0s 916us/step - loss: 0.7883 - root_mean_squared_error: 0.8878 - r_square: -0.0173 - val_loss: 0.7834 - val_root_mean_squared_error: 0.8851 - val_r_square: -0.0190\n",
      "Epoch 13/100\n",
      "46/46 [==============================] - 0s 872us/step - loss: 0.7886 - root_mean_squared_error: 0.8880 - r_square: -0.0208 - val_loss: 0.7819 - val_root_mean_squared_error: 0.8843 - val_r_square: -0.0159\n",
      "Epoch 14/100\n",
      "46/46 [==============================] - 0s 906us/step - loss: 0.7883 - root_mean_squared_error: 0.8879 - r_square: -0.0155 - val_loss: 0.7828 - val_root_mean_squared_error: 0.8847 - val_r_square: -0.0178\n",
      "Epoch 15/100\n",
      "46/46 [==============================] - 0s 941us/step - loss: 0.7879 - root_mean_squared_error: 0.8876 - r_square: -0.0164 - val_loss: 0.7819 - val_root_mean_squared_error: 0.8843 - val_r_square: -0.0157\n",
      "Epoch 16/100\n",
      "46/46 [==============================] - 0s 922us/step - loss: 0.7882 - root_mean_squared_error: 0.8878 - r_square: -0.0177 - val_loss: 0.7819 - val_root_mean_squared_error: 0.8843 - val_r_square: -0.0157\n",
      "Epoch 17/100\n",
      "46/46 [==============================] - 0s 930us/step - loss: 0.7883 - root_mean_squared_error: 0.8878 - r_square: -0.0160 - val_loss: 0.7826 - val_root_mean_squared_error: 0.8846 - val_r_square: -0.0174\n",
      "Epoch 18/100\n",
      "46/46 [==============================] - 0s 981us/step - loss: 0.7886 - root_mean_squared_error: 0.8880 - r_square: -0.0201 - val_loss: 0.7821 - val_root_mean_squared_error: 0.8844 - val_r_square: -0.0165\n",
      "Epoch 19/100\n",
      "46/46 [==============================] - 0s 965us/step - loss: 0.7879 - root_mean_squared_error: 0.8876 - r_square: -0.0161 - val_loss: 0.7841 - val_root_mean_squared_error: 0.8855 - val_r_square: -0.0201\n",
      "Epoch 20/100\n",
      "46/46 [==============================] - 0s 954us/step - loss: 0.7884 - root_mean_squared_error: 0.8879 - r_square: -0.0123 - val_loss: 0.7821 - val_root_mean_squared_error: 0.8844 - val_r_square: -0.0165\n",
      "Epoch 21/100\n",
      "46/46 [==============================] - 0s 995us/step - loss: 0.7881 - root_mean_squared_error: 0.8878 - r_square: -0.0148 - val_loss: 0.7833 - val_root_mean_squared_error: 0.8851 - val_r_square: -0.0188\n",
      "Epoch 22/100\n",
      "46/46 [==============================] - 0s 965us/step - loss: 0.7884 - root_mean_squared_error: 0.8879 - r_square: -0.0207 - val_loss: 0.7819 - val_root_mean_squared_error: 0.8843 - val_r_square: -0.0157\n",
      "Epoch 23/100\n",
      "46/46 [==============================] - 0s 902us/step - loss: 0.7881 - root_mean_squared_error: 0.8877 - r_square: -0.0161 - val_loss: 0.7827 - val_root_mean_squared_error: 0.8847 - val_r_square: -0.0176\n",
      "Epoch 24/100\n",
      "46/46 [==============================] - 0s 943us/step - loss: 0.7879 - root_mean_squared_error: 0.8876 - r_square: -0.0101 - val_loss: 0.7822 - val_root_mean_squared_error: 0.8844 - val_r_square: -0.0166\n",
      "Epoch 25/100\n",
      "46/46 [==============================] - 0s 919us/step - loss: 0.7881 - root_mean_squared_error: 0.8877 - r_square: -0.0112 - val_loss: 0.7820 - val_root_mean_squared_error: 0.8843 - val_r_square: -0.0161\n",
      "Epoch 26/100\n",
      "46/46 [==============================] - 0s 934us/step - loss: 0.7881 - root_mean_squared_error: 0.8877 - r_square: -0.0144 - val_loss: 0.7827 - val_root_mean_squared_error: 0.8847 - val_r_square: -0.0177\n",
      "Epoch 27/100\n",
      "46/46 [==============================] - 0s 944us/step - loss: 0.7880 - root_mean_squared_error: 0.8877 - r_square: -0.0165 - val_loss: 0.7825 - val_root_mean_squared_error: 0.8846 - val_r_square: -0.0172\n",
      "Epoch 28/100\n",
      "46/46 [==============================] - 0s 983us/step - loss: 0.7880 - root_mean_squared_error: 0.8877 - r_square: -0.0113 - val_loss: 0.7824 - val_root_mean_squared_error: 0.8845 - val_r_square: -0.0170\n",
      "Epoch 29/100\n",
      "46/46 [==============================] - 0s 933us/step - loss: 0.7886 - root_mean_squared_error: 0.8880 - r_square: -0.0262 - val_loss: 0.7826 - val_root_mean_squared_error: 0.8846 - val_r_square: -0.0174\n",
      "Epoch 30/100\n",
      "46/46 [==============================] - 0s 977us/step - loss: 0.7885 - root_mean_squared_error: 0.8880 - r_square: -0.0200 - val_loss: 0.7821 - val_root_mean_squared_error: 0.8844 - val_r_square: -0.0164\n",
      "Epoch 31/100\n",
      "46/46 [==============================] - 0s 940us/step - loss: 0.7878 - root_mean_squared_error: 0.8876 - r_square: -0.0124 - val_loss: 0.7828 - val_root_mean_squared_error: 0.8848 - val_r_square: -0.0179\n",
      "Epoch 32/100\n",
      "46/46 [==============================] - 0s 967us/step - loss: 0.7887 - root_mean_squared_error: 0.8881 - r_square: -0.0219 - val_loss: 0.7821 - val_root_mean_squared_error: 0.8844 - val_r_square: -0.0165\n",
      "Epoch 33/100\n",
      "46/46 [==============================] - 0s 965us/step - loss: 0.7883 - root_mean_squared_error: 0.8879 - r_square: -0.0178 - val_loss: 0.7820 - val_root_mean_squared_error: 0.8843 - val_r_square: -0.0160\n",
      "Epoch 34/100\n",
      "46/46 [==============================] - 0s 932us/step - loss: 0.7883 - root_mean_squared_error: 0.8879 - r_square: -0.0153 - val_loss: 0.7819 - val_root_mean_squared_error: 0.8843 - val_r_square: -0.0158\n",
      "Epoch 35/100\n",
      "46/46 [==============================] - 0s 983us/step - loss: 0.7883 - root_mean_squared_error: 0.8878 - r_square: -0.0174 - val_loss: 0.7823 - val_root_mean_squared_error: 0.8845 - val_r_square: -0.0169\n",
      "Epoch 36/100\n",
      "46/46 [==============================] - 0s 954us/step - loss: 0.7883 - root_mean_squared_error: 0.8879 - r_square: -0.0159 - val_loss: 0.7822 - val_root_mean_squared_error: 0.8844 - val_r_square: -0.0166\n",
      "Epoch 37/100\n",
      "46/46 [==============================] - 0s 925us/step - loss: 0.7879 - root_mean_squared_error: 0.8876 - r_square: -0.0141 - val_loss: 0.7829 - val_root_mean_squared_error: 0.8848 - val_r_square: -0.0180\n",
      "Epoch 38/100\n",
      "46/46 [==============================] - 0s 946us/step - loss: 0.7887 - root_mean_squared_error: 0.8881 - r_square: -0.0251 - val_loss: 0.7833 - val_root_mean_squared_error: 0.8850 - val_r_square: -0.0187\n",
      "Epoch 39/100\n",
      "46/46 [==============================] - 0s 1ms/step - loss: 0.7885 - root_mean_squared_error: 0.8879 - r_square: -0.0178 - val_loss: 0.7833 - val_root_mean_squared_error: 0.8850 - val_r_square: -0.0187\n",
      "Epoch 40/100\n",
      "46/46 [==============================] - 0s 1ms/step - loss: 0.7883 - root_mean_squared_error: 0.8879 - r_square: -0.0141 - val_loss: 0.7821 - val_root_mean_squared_error: 0.8843 - val_r_square: -0.0163\n",
      "Epoch 41/100\n",
      "46/46 [==============================] - 0s 901us/step - loss: 0.7884 - root_mean_squared_error: 0.8879 - r_square: -0.0169 - val_loss: 0.7820 - val_root_mean_squared_error: 0.8843 - val_r_square: -0.0162\n",
      "Epoch 42/100\n",
      "46/46 [==============================] - 0s 902us/step - loss: 0.7881 - root_mean_squared_error: 0.8878 - r_square: -0.0149 - val_loss: 0.7828 - val_root_mean_squared_error: 0.8848 - val_r_square: -0.0179\n",
      "Epoch 43/100\n",
      "46/46 [==============================] - 0s 933us/step - loss: 0.7880 - root_mean_squared_error: 0.8877 - r_square: -0.0143 - val_loss: 0.7820 - val_root_mean_squared_error: 0.8843 - val_r_square: -0.0162\n",
      "Epoch 44/100\n",
      "46/46 [==============================] - 0s 934us/step - loss: 0.7884 - root_mean_squared_error: 0.8879 - r_square: -0.0205 - val_loss: 0.7823 - val_root_mean_squared_error: 0.8845 - val_r_square: -0.0169\n",
      "Epoch 45/100\n",
      "46/46 [==============================] - 0s 932us/step - loss: 0.7888 - root_mean_squared_error: 0.8881 - r_square: -0.0253 - val_loss: 0.7824 - val_root_mean_squared_error: 0.8845 - val_r_square: -0.0170\n",
      "Epoch 46/100\n",
      "46/46 [==============================] - 0s 910us/step - loss: 0.7884 - root_mean_squared_error: 0.8879 - r_square: -0.0163 - val_loss: 0.7824 - val_root_mean_squared_error: 0.8845 - val_r_square: -0.0170\n",
      "Epoch 47/100\n",
      "46/46 [==============================] - 0s 878us/step - loss: 0.7883 - root_mean_squared_error: 0.8878 - r_square: -0.0178 - val_loss: 0.7820 - val_root_mean_squared_error: 0.8843 - val_r_square: -0.0161\n",
      "Epoch 48/100\n",
      "46/46 [==============================] - 0s 1ms/step - loss: 0.7874 - root_mean_squared_error: 0.8873 - r_square: -0.0142 - val_loss: 0.7827 - val_root_mean_squared_error: 0.8847 - val_r_square: -0.0158\n",
      "Epoch 49/100\n",
      "46/46 [==============================] - 0s 1ms/step - loss: 0.7884 - root_mean_squared_error: 0.8879 - r_square: -0.0151 - val_loss: 0.7823 - val_root_mean_squared_error: 0.8845 - val_r_square: -0.0169\n",
      "Epoch 50/100\n",
      "46/46 [==============================] - 0s 973us/step - loss: 0.7881 - root_mean_squared_error: 0.8877 - r_square: -0.0121 - val_loss: 0.7823 - val_root_mean_squared_error: 0.8845 - val_r_square: -0.0169\n",
      "Epoch 51/100\n",
      "46/46 [==============================] - 0s 943us/step - loss: 0.7882 - root_mean_squared_error: 0.8878 - r_square: -0.0133 - val_loss: 0.7824 - val_root_mean_squared_error: 0.8845 - val_r_square: -0.0171\n",
      "Epoch 52/100\n",
      "46/46 [==============================] - 0s 930us/step - loss: 0.7884 - root_mean_squared_error: 0.8879 - r_square: -0.0192 - val_loss: 0.7819 - val_root_mean_squared_error: 0.8843 - val_r_square: -0.0156\n",
      "Epoch 53/100\n",
      "46/46 [==============================] - 0s 964us/step - loss: 0.7880 - root_mean_squared_error: 0.8877 - r_square: -0.0147 - val_loss: 0.7836 - val_root_mean_squared_error: 0.8852 - val_r_square: -0.0193\n",
      "Epoch 54/100\n",
      "46/46 [==============================] - 0s 901us/step - loss: 0.7882 - root_mean_squared_error: 0.8878 - r_square: -0.0157 - val_loss: 0.7838 - val_root_mean_squared_error: 0.8853 - val_r_square: -0.0197\n",
      "Epoch 55/100\n",
      "46/46 [==============================] - 0s 913us/step - loss: 0.7875 - root_mean_squared_error: 0.8874 - r_square: -0.0179 - val_loss: 0.7820 - val_root_mean_squared_error: 0.8843 - val_r_square: -0.0155\n",
      "Epoch 56/100\n",
      "46/46 [==============================] - 0s 967us/step - loss: 0.7886 - root_mean_squared_error: 0.8880 - r_square: -0.0248 - val_loss: 0.7820 - val_root_mean_squared_error: 0.8843 - val_r_square: -0.0155\n",
      "Epoch 57/100\n",
      "46/46 [==============================] - 0s 941us/step - loss: 0.7883 - root_mean_squared_error: 0.8879 - r_square: -0.0132 - val_loss: 0.7821 - val_root_mean_squared_error: 0.8844 - val_r_square: -0.0164\n",
      "Epoch 58/100\n",
      "46/46 [==============================] - 0s 927us/step - loss: 0.7883 - root_mean_squared_error: 0.8879 - r_square: -0.0193 - val_loss: 0.7819 - val_root_mean_squared_error: 0.8843 - val_r_square: -0.0156\n",
      "Epoch 59/100\n",
      "46/46 [==============================] - 0s 947us/step - loss: 0.7888 - root_mean_squared_error: 0.8881 - r_square: -0.0214 - val_loss: 0.7821 - val_root_mean_squared_error: 0.8843 - val_r_square: -0.0163\n",
      "Epoch 60/100\n",
      "46/46 [==============================] - 0s 1ms/step - loss: 0.7884 - root_mean_squared_error: 0.8879 - r_square: -0.0215 - val_loss: 0.7829 - val_root_mean_squared_error: 0.8848 - val_r_square: -0.0181\n",
      "Epoch 61/100\n",
      "46/46 [==============================] - 0s 939us/step - loss: 0.7886 - root_mean_squared_error: 0.8880 - r_square: -0.0198 - val_loss: 0.7822 - val_root_mean_squared_error: 0.8844 - val_r_square: -0.0167\n",
      "Epoch 62/100\n",
      "46/46 [==============================] - 0s 940us/step - loss: 0.7883 - root_mean_squared_error: 0.8879 - r_square: -0.0168 - val_loss: 0.7822 - val_root_mean_squared_error: 0.8844 - val_r_square: -0.0166\n",
      "Epoch 63/100\n",
      "46/46 [==============================] - 0s 1ms/step - loss: 0.7882 - root_mean_squared_error: 0.8878 - r_square: -0.0150 - val_loss: 0.7819 - val_root_mean_squared_error: 0.8843 - val_r_square: -0.0156\n",
      "Epoch 64/100\n",
      "46/46 [==============================] - 0s 910us/step - loss: 0.7882 - root_mean_squared_error: 0.8878 - r_square: -0.0141 - val_loss: 0.7823 - val_root_mean_squared_error: 0.8845 - val_r_square: -0.0168\n",
      "Epoch 65/100\n",
      "46/46 [==============================] - 0s 930us/step - loss: 0.7878 - root_mean_squared_error: 0.8876 - r_square: -0.0192 - val_loss: 0.7823 - val_root_mean_squared_error: 0.8845 - val_r_square: -0.0156\n",
      "Epoch 66/100\n",
      "46/46 [==============================] - 0s 898us/step - loss: 0.7885 - root_mean_squared_error: 0.8880 - r_square: -0.0173 - val_loss: 0.7821 - val_root_mean_squared_error: 0.8844 - val_r_square: -0.0165\n",
      "Epoch 67/100\n",
      "46/46 [==============================] - 0s 901us/step - loss: 0.7882 - root_mean_squared_error: 0.8878 - r_square: -0.0139 - val_loss: 0.7822 - val_root_mean_squared_error: 0.8844 - val_r_square: -0.0167\n",
      "Epoch 68/100\n",
      "46/46 [==============================] - 0s 868us/step - loss: 0.7883 - root_mean_squared_error: 0.8879 - r_square: -0.0175 - val_loss: 0.7824 - val_root_mean_squared_error: 0.8845 - val_r_square: -0.0171\n",
      "Epoch 69/100\n",
      "46/46 [==============================] - 0s 882us/step - loss: 0.7884 - root_mean_squared_error: 0.8879 - r_square: -0.0197 - val_loss: 0.7821 - val_root_mean_squared_error: 0.8843 - val_r_square: -0.0163\n",
      "Epoch 70/100\n",
      "46/46 [==============================] - 0s 888us/step - loss: 0.7882 - root_mean_squared_error: 0.8878 - r_square: -0.0145 - val_loss: 0.7821 - val_root_mean_squared_error: 0.8844 - val_r_square: -0.0165\n",
      "Epoch 71/100\n",
      "46/46 [==============================] - 0s 878us/step - loss: 0.7881 - root_mean_squared_error: 0.8878 - r_square: -0.0119 - val_loss: 0.7823 - val_root_mean_squared_error: 0.8845 - val_r_square: -0.0169\n",
      "Epoch 72/100\n",
      "46/46 [==============================] - 0s 885us/step - loss: 0.7873 - root_mean_squared_error: 0.8873 - r_square: -0.0174 - val_loss: 0.7852 - val_root_mean_squared_error: 0.8861 - val_r_square: -0.0220\n",
      "Epoch 73/100\n",
      "46/46 [==============================] - 0s 906us/step - loss: 0.7882 - root_mean_squared_error: 0.8878 - r_square: -0.0102 - val_loss: 0.7823 - val_root_mean_squared_error: 0.8845 - val_r_square: -0.0168\n",
      "Epoch 74/100\n",
      "46/46 [==============================] - 0s 850us/step - loss: 0.7882 - root_mean_squared_error: 0.8878 - r_square: -0.0184 - val_loss: 0.7824 - val_root_mean_squared_error: 0.8845 - val_r_square: -0.0170\n",
      "Epoch 75/100\n",
      "46/46 [==============================] - 0s 881us/step - loss: 0.7880 - root_mean_squared_error: 0.8877 - r_square: -0.0138 - val_loss: 0.7820 - val_root_mean_squared_error: 0.8843 - val_r_square: -0.0155\n",
      "Epoch 76/100\n",
      "46/46 [==============================] - 0s 901us/step - loss: 0.7885 - root_mean_squared_error: 0.8880 - r_square: -0.0219 - val_loss: 0.7820 - val_root_mean_squared_error: 0.8843 - val_r_square: -0.0155\n",
      "Epoch 77/100\n",
      "46/46 [==============================] - 0s 929us/step - loss: 0.7882 - root_mean_squared_error: 0.8878 - r_square: -0.0158 - val_loss: 0.7822 - val_root_mean_squared_error: 0.8844 - val_r_square: -0.0166\n",
      "Epoch 78/100\n",
      "46/46 [==============================] - 0s 928us/step - loss: 0.7879 - root_mean_squared_error: 0.8877 - r_square: -0.0136 - val_loss: 0.7820 - val_root_mean_squared_error: 0.8843 - val_r_square: -0.0155\n",
      "Epoch 79/100\n",
      "46/46 [==============================] - 0s 910us/step - loss: 0.7882 - root_mean_squared_error: 0.8878 - r_square: -0.0147 - val_loss: 0.7830 - val_root_mean_squared_error: 0.8849 - val_r_square: -0.0182\n",
      "Epoch 80/100\n",
      "46/46 [==============================] - 0s 876us/step - loss: 0.7879 - root_mean_squared_error: 0.8876 - r_square: -0.0179 - val_loss: 0.7820 - val_root_mean_squared_error: 0.8843 - val_r_square: -0.0155\n",
      "Epoch 81/100\n",
      "46/46 [==============================] - 0s 911us/step - loss: 0.7881 - root_mean_squared_error: 0.8877 - r_square: -0.0167 - val_loss: 0.7819 - val_root_mean_squared_error: 0.8843 - val_r_square: -0.0159\n",
      "Epoch 82/100\n",
      "46/46 [==============================] - 0s 899us/step - loss: 0.7884 - root_mean_squared_error: 0.8879 - r_square: -0.0173 - val_loss: 0.7826 - val_root_mean_squared_error: 0.8846 - val_r_square: -0.0174\n",
      "Epoch 83/100\n",
      "46/46 [==============================] - 0s 909us/step - loss: 0.7882 - root_mean_squared_error: 0.8878 - r_square: -0.0133 - val_loss: 0.7820 - val_root_mean_squared_error: 0.8843 - val_r_square: -0.0161\n",
      "Epoch 84/100\n",
      "46/46 [==============================] - 0s 883us/step - loss: 0.7883 - root_mean_squared_error: 0.8878 - r_square: -0.0160 - val_loss: 0.7826 - val_root_mean_squared_error: 0.8846 - val_r_square: -0.0174\n",
      "Epoch 85/100\n",
      "46/46 [==============================] - 0s 889us/step - loss: 0.7882 - root_mean_squared_error: 0.8878 - r_square: -0.0234 - val_loss: 0.7819 - val_root_mean_squared_error: 0.8843 - val_r_square: -0.0159\n",
      "Epoch 86/100\n",
      "46/46 [==============================] - 0s 860us/step - loss: 0.7884 - root_mean_squared_error: 0.8879 - r_square: -0.0174 - val_loss: 0.7823 - val_root_mean_squared_error: 0.8845 - val_r_square: -0.0169\n",
      "Epoch 87/100\n",
      "46/46 [==============================] - 0s 890us/step - loss: 0.7882 - root_mean_squared_error: 0.8878 - r_square: -0.0158 - val_loss: 0.7822 - val_root_mean_squared_error: 0.8844 - val_r_square: -0.0167\n",
      "Epoch 88/100\n",
      "46/46 [==============================] - 0s 874us/step - loss: 0.7884 - root_mean_squared_error: 0.8879 - r_square: -0.0185 - val_loss: 0.7820 - val_root_mean_squared_error: 0.8843 - val_r_square: -0.0160\n",
      "Epoch 89/100\n",
      "46/46 [==============================] - 0s 902us/step - loss: 0.7882 - root_mean_squared_error: 0.8878 - r_square: -0.0175 - val_loss: 0.7825 - val_root_mean_squared_error: 0.8846 - val_r_square: -0.0172\n",
      "Epoch 90/100\n",
      "46/46 [==============================] - 0s 932us/step - loss: 0.7882 - root_mean_squared_error: 0.8878 - r_square: -0.0151 - val_loss: 0.7821 - val_root_mean_squared_error: 0.8844 - val_r_square: -0.0164\n",
      "Epoch 91/100\n",
      "46/46 [==============================] - 0s 892us/step - loss: 0.7883 - root_mean_squared_error: 0.8878 - r_square: -0.0209 - val_loss: 0.7830 - val_root_mean_squared_error: 0.8849 - val_r_square: -0.0182\n",
      "Epoch 92/100\n",
      "46/46 [==============================] - 0s 890us/step - loss: 0.7885 - root_mean_squared_error: 0.8880 - r_square: -0.0196 - val_loss: 0.7826 - val_root_mean_squared_error: 0.8847 - val_r_square: -0.0175\n",
      "Epoch 93/100\n",
      "46/46 [==============================] - 0s 904us/step - loss: 0.7886 - root_mean_squared_error: 0.8880 - r_square: -0.0198 - val_loss: 0.7820 - val_root_mean_squared_error: 0.8843 - val_r_square: -0.0161\n",
      "Epoch 94/100\n",
      "46/46 [==============================] - 0s 901us/step - loss: 0.7872 - root_mean_squared_error: 0.8872 - r_square: -0.0248 - val_loss: 0.7833 - val_root_mean_squared_error: 0.8851 - val_r_square: -0.0164\n",
      "Epoch 95/100\n",
      "46/46 [==============================] - 0s 883us/step - loss: 0.7885 - root_mean_squared_error: 0.8880 - r_square: -0.0130 - val_loss: 0.7819 - val_root_mean_squared_error: 0.8843 - val_r_square: -0.0158\n",
      "Epoch 96/100\n",
      "46/46 [==============================] - 0s 915us/step - loss: 0.7880 - root_mean_squared_error: 0.8877 - r_square: -0.0111 - val_loss: 0.7819 - val_root_mean_squared_error: 0.8843 - val_r_square: -0.0157\n",
      "Epoch 97/100\n",
      "46/46 [==============================] - 0s 919us/step - loss: 0.7885 - root_mean_squared_error: 0.8879 - r_square: -0.0166 - val_loss: 0.7820 - val_root_mean_squared_error: 0.8843 - val_r_square: -0.0161\n",
      "Epoch 98/100\n",
      "46/46 [==============================] - 0s 875us/step - loss: 0.7879 - root_mean_squared_error: 0.8876 - r_square: -0.0143 - val_loss: 0.7820 - val_root_mean_squared_error: 0.8843 - val_r_square: -0.0162\n",
      "Epoch 99/100\n",
      "46/46 [==============================] - 0s 928us/step - loss: 0.7880 - root_mean_squared_error: 0.8877 - r_square: -0.0181 - val_loss: 0.7828 - val_root_mean_squared_error: 0.8848 - val_r_square: -0.0178\n",
      "Epoch 100/100\n",
      "46/46 [==============================] - 0s 894us/step - loss: 0.7881 - root_mean_squared_error: 0.8877 - r_square: -0.0100 - val_loss: 0.7820 - val_root_mean_squared_error: 0.8843 - val_r_square: -0.0162\n",
      "12/12 [==============================] - 0s 610us/step - loss: 0.7820 - root_mean_squared_error: 0.8843 - r_square: -0.0162\n",
      "[0.7820200324058533, 0.8843189477920532, -0.01617300510406494]\n",
      "23/23 [==============================] - 0s 451us/step\n",
      "INFO:tensorflow:Assets written to: /var/folders/83/j83q5_mj11956_7k9tnld0ym0000gn/T/tmpmou9z1nv/model/data/model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/83/j83q5_mj11956_7k9tnld0ym0000gn/T/tmpmou9z1nv/model/data/model/assets\n",
      "2024/04/23 11:18:31 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /var/folders/83/j83q5_mj11956_7k9tnld0ym0000gn/T/tmpmou9z1nv/model, flavor: tensorflow). Fall back to return ['tensorflow==2.13.0', 'cloudpickle==3.0.0']. Set logging level to DEBUG to see the full traceback. \n"
     ]
    }
   ],
   "source": [
    "mlflow.set_experiment(\"WineQuality_R03\")\n",
    "with mlflow.start_run():\n",
    "    # Define the search space\n",
    "    params = {\n",
    "                \"lr\": np.random.choice(np.linspace(0.001, 0.1, 10)), \n",
    "                \"momentum\": np.random.choice(np.linspace(0.1, 0.99, 10)), \n",
    "                \"epochs\": np.random.choice([10, 100, 500, 1000]),\n",
    "                \"num_layers\": np.random.choice([2, 3, 4, 5]),\n",
    "                \"batch_size\": np.random.choice([32, 64, 128, 256]),\n",
    "                \"hidden_size\": np.random.choice([1, 2, 3, 4, 5])\n",
    "            }\n",
    "    print(params)\n",
    "    # data dict\n",
    "    data = {\n",
    "            'train_x': train_x, 'train_y': train_y,\n",
    "            'valid_x': valid_x, 'valid_y': valid_y,\n",
    "            'test_x': test_x, 'test_y': test_y\n",
    "            }\n",
    "    # store results\n",
    "    result = train_model(params, data=data)\n",
    "\n",
    "    signature = infer_signature(valid_x, result['model'].predict(valid_x))\n",
    "\n",
    "    # Log the best parameters, loss, and model\n",
    "    mlflow.log_params(params)\n",
    "    mlflow.log_metric(\"eval_rmse\", result[\"loss\"])\n",
    "    mlflow.log_metric(\"eval_r2\", result[\"R2\"])\n",
    "    mlflow.tensorflow.log_model(model = result[\"model\"], \n",
    "                                artifact_path=\"model\", \n",
    "                                signature=signature)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MatML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
